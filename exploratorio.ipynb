{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis exploratorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesado común"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\mario\\anaconda3\\envs\\spacy-env\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\mario\\anaconda3\\envs\\spacy-env\\lib\\site-packages (from nltk) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mario\\anaconda3\\envs\\spacy-env\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mario\\anaconda3\\envs\\spacy-env\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mario\\anaconda3\\envs\\spacy-env\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mario\\anaconda3\\envs\\spacy-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # read the csv\n",
    "import re # regex to detect username, url, html entity\n",
    "import nltk # to use word tokenize (split the sentence into words)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords # to remove the stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import kagglehub\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "import json\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Inicializar el lematizador\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Borrar entidades HTML\n",
    "def remove_entity(raw_text):\n",
    "    entity_regex = r\"&[^\\s;]+;\"\n",
    "    return re.sub(entity_regex, \"\", raw_text)\n",
    "\n",
    "# Reemplazar menciones por 'user'\n",
    "def change_user(raw_text):\n",
    "    regex = r\"@([^ ]+)\"\n",
    "    return re.sub(regex, \"user\", raw_text)\n",
    "\n",
    "def remove_url(raw_text):\n",
    "    url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    text = re.sub(url_regex, '', raw_text)\n",
    "    # Eliminar posibles instancias de 'href', 'http', o 'www' restantes\n",
    "    text = re.sub(r'\\b(http|www|href)\\b', '', text)\n",
    "    return text\n",
    "\n",
    "# Borrar signos de puntuación\n",
    "def remove_noise_symbols(raw_text):\n",
    "    text = raw_text.replace('\"', '').replace(\"'\", '').replace(\"!\", '').replace(\"`\", '').replace(\"..\", '')\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# Borrar \"RT\"\n",
    "def remove_rt(raw_text):\n",
    "    return re.sub(r'\\bRT\\b', '', raw_text, flags=re.IGNORECASE)\n",
    "\n",
    "# Borrar \"user\"\n",
    "def remove_user(raw_text):\n",
    "    return re.sub(r'\\buser\\b', '', raw_text, flags=re.IGNORECASE)\n",
    "\n",
    "# Preprocesamiento principal para un solo texto\n",
    "def preprocess(text):\n",
    "    text = change_user(text)\n",
    "    text = remove_entity(text)\n",
    "    text = remove_url(text)\n",
    "    text = remove_noise_symbols(text)\n",
    "    text = remove_rt(text)\n",
    "    text = remove_user(text)\n",
    "    return text.lower()\n",
    "\n",
    "# Tokenizar sin lematizar\n",
    "def tokenize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = preprocess(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Lematización por separado\n",
    "def lemmatize_tokens(tokens, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "def process_text_column(df, text_column, lemmatizer=None):\n",
    "    df['tokens'] = df[text_column].apply(tokenize_text)\n",
    "    if lemmatizer is not None:\n",
    "        df['tokens'] = df['tokens'].apply(lambda tokens: lemmatize_tokens(tokens, lemmatizer))\n",
    "    return df\n",
    "\n",
    "def eliminar_stopwords(df, column_name):\n",
    "    # Aplicar eliminación de stopwords a la columna especificada\n",
    "    df[column_name] = df[column_name].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "    return df\n",
    "\n",
    "# función para calcular el porcentaje de stopwords en un texto\n",
    "def stopword_percentage(tokens):\n",
    "    stopword_count = sum(1 for word in tokens if word in stop_words)\n",
    "    stopword_pct = (stopword_count / len(tokens)) * 100\n",
    "    return stopword_pct\n",
    "\n",
    "# Función para limpiar, tokenizar y lematizar\n",
    "def process_text_column2(df, text_column):\n",
    "    # Función interna para limpiar, tokenizar y lematizar\n",
    "\n",
    "    # Aplicar la tokenización a la columna de texto\n",
    "    df['stopword_percentage'] = df[text_column].apply(stopword_percentage)\n",
    "    return df\n",
    "\n",
    "def graphics_stopwords(df, text_column):\n",
    "\n",
    "    stopwords_by_rating = df.groupby(text_column)['stopword_percentage'].mean()\n",
    "    meaningful_by_rating = 100 - stopwords_by_rating\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    bar1 = plt.bar(stopwords_by_rating.index, stopwords_by_rating, color='#2C77A8', label='Stopwords')\n",
    "    bar2 = plt.bar(meaningful_by_rating.index, meaningful_by_rating, bottom=stopwords_by_rating, color='#87CEEB', label='Palabras con significado')\n",
    "\n",
    "    plt.xlabel(text_column)\n",
    "    plt.ylabel('Porcentaje')\n",
    "    plt.title('Porcentaje de stopwords vs. con significado')\n",
    "    plt.xticks(stopwords_by_rating.index)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def graficar_longitud_textos(df, text_column, rating_column):\n",
    "    df['NP'] = df[text_column].apply(len)\n",
    "\n",
    "    ratings = sorted(df[rating_column].unique())\n",
    "    num_ratings = len(ratings)\n",
    "    num_cols = min(3, num_ratings + 1)\n",
    "    num_rows = (num_ratings + 1 + num_cols - 1) // num_cols\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(4 * num_cols, 4 * num_rows))\n",
    "    axes = np.ravel(axes)\n",
    "\n",
    "    for i, rating in enumerate(ratings):\n",
    "        ax = axes[i]\n",
    "        subset = df[df[rating_column] == rating]\n",
    "        media_np = subset['NP'].mean()\n",
    "        max_np = subset['NP'].max()\n",
    "\n",
    "        ax.hist(subset['NP'], bins=30, edgecolor='black', alpha=0.7, color='#2C77A8')\n",
    "        ax.set_xlim(0, max_np + 5)  # Añadimos un margen para mejor visualización\n",
    "        ax.set_xlabel('Número de palabras')\n",
    "        ax.set_ylabel('Frecuencia')\n",
    "        ax.set_title(f'Rating {rating}')\n",
    "\n",
    "        ax.text(max_np * 0.5, ax.get_ylim()[1] * 0.8, f'Media: {media_np:.1f}', fontsize=12, color='red')\n",
    "\n",
    "    # Gráfico global al final\n",
    "    global_ax = axes[num_ratings]\n",
    "    media_global = df['NP'].mean()\n",
    "    max_np_global = df['NP'].max()\n",
    "\n",
    "    global_ax.hist(df['NP'], bins=30, edgecolor='black', alpha=0.7, color='#2C77A8')\n",
    "    global_ax.set_xlim(0, max_np_global + 5)\n",
    "    global_ax.set_xlabel('Número de palabras')\n",
    "    global_ax.set_ylabel('Frecuencia')\n",
    "    global_ax.set_title('Distribución Global')\n",
    "    global_ax.text(max_np_global * 0.3, global_ax.get_ylim()[1] * 0.8, f'Media: {media_global:.1f}', fontsize=12, color='red')\n",
    "\n",
    "    # Eliminar subplots vacíos si hay\n",
    "    for j in range(num_ratings + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def wordclouds(df, column_partition, top_n):\n",
    "\n",
    "    # Obtener lista de todas las palabras en el dataset\n",
    "    all_words = [word for tokens in df['tokens'] for word in tokens]\n",
    "\n",
    "    # Contar frecuencia de palabras\n",
    "    word_freq = Counter(all_words)\n",
    "\n",
    "    # Obtener las `top_n` palabras más comunes\n",
    "    top_words = {word for word, _ in word_freq.most_common(top_n)}\n",
    "\n",
    "    # Filtrar los tokens eliminando las palabras más comunes\n",
    "    df['filtered_tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word not in top_words])\n",
    "\n",
    "    # Generar WordCloud por cada rating\n",
    "    ratings = sorted(df[column_partition].unique())\n",
    "    fig, axes = plt.subplots(1, len(ratings), figsize=(20, 5))\n",
    "\n",
    "    for i, rating in enumerate(ratings):\n",
    "        text = ' '.join([' '.join(tokens) for tokens in df[df[column_partition] == rating]['filtered_tokens']])\n",
    "        wordcloud = WordCloud(width=400, height=400, background_color='white').generate(text)\n",
    "\n",
    "        # Mostrar en subplot\n",
    "        axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f'{rating}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Función para crear gráficos de barras horizontales con las palabras más importantes por Rating\n",
    "def tfidf(df, text_column, column_partition):\n",
    "\n",
    "    # Agrupar textos por rating\n",
    "    grouped_reviews = df.groupby(column_partition)[text_column].apply(lambda x: \" \".join([\" \".join(tokens) for tokens in x]))\n",
    "\n",
    "    # Vectorizar con TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(grouped_reviews)\n",
    "\n",
    "    # Convertir a DataFrame\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=grouped_reviews.index, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Seleccionar las palabras con mayor TF-IDF en cada rating\n",
    "    top_terms_per_rating = {}\n",
    "    for rating in tfidf_df.index:\n",
    "        top_terms = tfidf_df.loc[rating].nlargest(5)  # Tomar las 5 palabras con mayor TF-IDF\n",
    "        top_terms_per_rating[rating] = top_terms\n",
    "\n",
    "    # Graficar los resultados\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for i, (rating, terms) in enumerate(top_terms_per_rating.items(), 1):\n",
    "        plt.subplot(2, 3, i)  # Ajustar el número de filas y columnas según la cantidad de ratings\n",
    "        terms.plot(kind='barh', color='royalblue', ax=plt.gca())  # Graficar las top terms como barras horizontales\n",
    "        plt.title(f'Top TF-IDF - {rating}')\n",
    "        plt.xlabel('TF-IDF')\n",
    "        plt.ylabel('Palabras')\n",
    "        plt.gca().invert_yaxis()  # Para que la barra más alta esté arriba\n",
    "\n",
    "    plt.tight_layout()  # Ajustar el espacio entre los subgráficos\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Función para obtener n-gramas\n",
    "def get_ngrams(tokens, n):\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]\n",
    "\n",
    "# Función para calcular y graficar bigramas y trigramas más comunes por clases\n",
    "def plot_most_common_ngrams_by_class(df, text_column, class_column):\n",
    "    # Crear subgráficos para bigramas\n",
    "    num_classes = len(df[class_column].unique())\n",
    "    ncols = 3  # Número de columnas de subgráficos\n",
    "    nrows = (num_classes + ncols - 1) // ncols  # Calcular número de filas necesarias\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(15, 5 * nrows))\n",
    "    axes = axes.flatten()  # Aplanar la matriz de ejes para facilitar el acceso\n",
    "\n",
    "    # Generar bigramas\n",
    "    for i, class_value in enumerate(df[class_column].unique()):\n",
    "        # Filtrar los datos por clase\n",
    "        subset = df[df[class_column] == class_value]\n",
    "\n",
    "        # Obtener todos los tokens del subset filtrado\n",
    "        all_words = [word for tokens in subset[text_column] for word in tokens]\n",
    "\n",
    "        # Calcular bigramas\n",
    "        bigram_freq = Counter(get_ngrams(all_words, 2))\n",
    "\n",
    "        # Obtener los 10 bigramas más comunes\n",
    "        bigram_most_common = bigram_freq.most_common(10)\n",
    "\n",
    "        # Bigramas\n",
    "        bigrams, bigram_counts = zip(*bigram_most_common)\n",
    "        axes[i].barh(bigrams, bigram_counts, color='#a9dfd0', alpha=0.7)\n",
    "\n",
    "        # Configuración de los gráficos\n",
    "        axes[i].set_title(f'Bigramas más comunes en {class_value}')\n",
    "        axes[i].set_xlabel('Frecuencia')\n",
    "        axes[i].set_ylabel('Bigramas')\n",
    "\n",
    "    # Ajustar espacio entre subgráficos y mostrar el plot de bigramas\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Crear subgráficos para trigramas\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(15, 5 * nrows))\n",
    "    axes = axes.flatten()  # Aplanar la matriz de ejes para facilitar el acceso\n",
    "\n",
    "    # Generar trigramas\n",
    "    for i, class_value in enumerate(df[class_column].unique()):\n",
    "        # Filtrar los datos por clase\n",
    "        subset = df[df[class_column] == class_value]\n",
    "\n",
    "        # Obtener todos los tokens del subset filtrado\n",
    "        all_words = [word for tokens in subset[text_column] for word in tokens]\n",
    "\n",
    "        # Calcular trigramas\n",
    "        trigram_freq = Counter(get_ngrams(all_words, 3))\n",
    "\n",
    "        # Obtener los 10 trigramas más comunes\n",
    "        trigram_most_common = trigram_freq.most_common(10)\n",
    "\n",
    "        # Trigramas\n",
    "        trigrams, trigram_counts = zip(*trigram_most_common)\n",
    "        axes[i].barh(trigrams, trigram_counts, color='#2C77A8', alpha=0.7)\n",
    "\n",
    "        # Configuración de los gráficos\n",
    "        axes[i].set_title(f'Trigramas más comunes en {class_value}')\n",
    "        axes[i].set_xlabel('Frecuencia')\n",
    "        axes[i].set_ylabel('Trigramas')\n",
    "\n",
    "    # Ajustar espacio entre subgráficos y mostrar el plot de trigramas\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m process_text_column2(\u001b[43mdf\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m graphics_stopwords(df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "process_text_column2(df, 'text')\n",
    "graphics_stopwords(df, 'label')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
